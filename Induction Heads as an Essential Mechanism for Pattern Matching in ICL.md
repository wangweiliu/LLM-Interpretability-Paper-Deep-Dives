# 论文精读调研

> 调研论文，并撰写下列内容
> 调研人：王伟柳

## 论文名称

名称：Induction Heads as an Essential Mechanism for Pattern Matching in
In-context Learning

链接：https://aclanthology.org/2025.findings-naacl.283.pdf

## 基础信息

| 工作类别（攻击/可解释性分析/防御）       | 具体信息 |
| ---------------------------------------- | -------- |
| 模态（llm/推理llm/多模态）               |llm          |
| 时间（年份，arxiv文章需要月份，如25.11） |2025          |
| 刊物                                     |NACCL          |
| 首作者                                   |Joy Crosbie|
| 首作单位                                 |University of Amsterdam|
| 研究目标（输出增长/循环输出）            |****|
| 一句话方法概括                           |首先通过前缀匹配分数来确定归纳头的位置，然后通过消融实验来对比有无归纳头在各种任务上模型的表现变化|

## 方法概述

> 1. 自己阅读文章，不要使用ai总结，有看不懂的地方可以问模型，但是不能依靠模型阅读。
> 2. 方法概述不需要过于详细的形式化公式，使用语言表述，以【研究目标-研究手段】为线路进行总结。其中研究手段可以稍作展开，分点按步骤进行撰写。
### 研究目标
1. 确定Induction Heads的位置（哪一层的哪个头）
2. 调查Induction Heads对于特定ICL任务的重要性以及更好地理解Induction Heads在few-shot ICL中的作用。

### 研究手段
1. 通过对比头的prefix matching score来确定***behavioural*** induction heads（作者在文章中提到：In this work, the term "induction heads" refers to behavioural induction heads.）。

2. 对induction heads采用均值消融（mean ablation）。首先将构造好的的样本输入模型进行一次前向传播，在这个过程中，使用Pyvene代码库，把每一个注意力头内部的激活值都记录下来，然后计算均值；在推理阶段，将那些要消融的注意力头的激活值退换成刚刚得到的均值激活值。为了做对比，作者还对1%-3%的non induction heads也进行均值消融，同时确保这些头和之前消融的头在同一层。

3. 使用attention konckout技术。举例说明：已知输入[a][b]...[a],第一个[a]的位置记为r，第二个[a]的位置记为c，当模型推理到位置c时，作者为了防止注意力头参考位置r的注意力，将头关于那个位置的casual attention mask设置为-∞。

4. 基于1、2、3方法的Abstract Pattern Recognition Tasks实验。作者构造了7类任务，然后对模型进行完整模型、1%和3%归纳头均值消融、1%和3%随机头均值消融、1%和3%归纳头attention konckout操作，然后得到每种情况下的得分。

5. 基于1、2、3方法的NLP Tasks实验。其手段和 4. 类似

## 实验效果
> 概括介绍实验结果，主要关注作者采取的实验指标、baseline和实际效果。

模型选择：___Llama-3-8B___、___IntermLM2-20B___
1. 数据集构造：使用50个随机的tokens（排除4%最常见和4%最罕见的token）构成一个句子，然后将句子重复四遍构成序列。指标计算方法（prefix matching score）：针对每个Token，统计其对那些‘在早前的重复片段中紧跟在相同Token之后’的Token的注意力值，并计算这些值的平均数。最终的分数是基于五个随机序列的计算结果取平均得到的。实际效果：在模型的各个层中都分布了较高分数的头，llama模型中有3%的头分数大于等于0.3，有些甚至达到了0.98。

2. 数据集：MNLI训练集的前提句（Premise），将不同的前提句拼接起来，产生共500个样本，每个样本3000tokens。结果：将这些均值激活保存下来，为后续的实验做准备。

3. 此手段为后续的实验提供方法

4. 实验指标：回答一系列任务的结果正确率。baseline：随机选择模型头进行进行均值消融或attention knockout。实验结果：对模型的归纳头进行均值消融或attention knockout操作后，模型得分大大下降，而随机选择的结果得分也是下降，但没那么剧烈。同时zero-shot的得分也远低于few-shot（为了体现模型的ICL能力）。

5. 数据集：SuperGLUE、BoolQ、RTE、WIC、WSC、ETHOS、SST-2、SUBJ，都是二分类任务数据集，标签设置有两种，分别是yes/no和foo/bar（语义无关）。实验指标：ICL benefit（计算10 shot和 0 shot设置下模型得分情况变化百分比）。实验效果：切除归纳头相比于随机切除，使模型的表现下降更剧烈。在语义无关标签设置下，切除得分最高3%的归纳头相比于切除得分最高的1%归纳头表现并没有明显下降。


## 引用论文

> 关注论文的`Introduction`、`related work`和`experiment setup`部分，追踪查找作者列举的**关于资源消耗和输出长度异常**的相关研究。列出其基本信息

| 论文题目 | 发表刊物 | 时间 | 一句话概括研究领域和方向（例：llm 可解释性+防御） |
| -------- | -------- | ---- | ------------------------------------------------- |
||          |    |                                               |


## 被引论文
这篇文章一共就两篇被引

> 在 https://scholar.google.com/ 和 上进行当前论文搜索，以此查看所有引用本论文的研究，提取其中**关于资源消耗和输出长度异常**的相关研究。列出其基本信息

| 论文题目 | 发表刊物 | 时间 | 一句话概括研究领域和方向（例：llm 可解释性+防御） |
| -------- | -------- | ---- | ------------------------------------------------- |
|         |          |      |                         |
