# 论文精读调研

> 调研论文，并撰写下列内容
> 调研人：王伟柳

## 论文名称

名称：Interpreting the Repeated Token Phenomenon in Large Language Models

链接：https://icml.cc/virtual/2025/49619

## 基础信息

| 工作类别（攻击/可解释性分析/防御）       | 具体信息 |
| ---------------------------------------- | -------- |
| 模态（llm/推理llm/多模态）                 |llm       |
| 时间（年份，arxiv文章需要月份，如25.11）    |2025|
| 刊物                                     |ICML|
| 首作者                                   |Itay Yona|
| 首作单位                                 |Google DeepMind|
| 研究目标（输出增长/循环输出）              |循环输入|
| 一句话方法概括                            |提出了能够窃取模型训练信息的簇攻击，并给出了防御方法|

## 方法概述

> 1. 自己阅读文章，不要使用ai总结，有看不懂的地方可以问模型，但是不能依靠模型阅读。
> 2. 方法概述不需要过于详细的形式化公式，使用语言表述，以【研究目标-研究手段】为线路进行总结。其中研究手段可以稍作展开，分点按步骤进行撰写。
### 研究点1
#### 研究目标
寻找重复token输入导致模型发散和attention sink之间的关系
#### 研究手段
1、观察正常句子和重复token构成的句子的attention score矩阵。
2、观察重复token在隐藏层（浅层的）的L2范数大小随着重复次数的变化。
3、将隐藏层（浅层的）的部分的神经元做0消融（将值置为0），观察L2范数大小变化。
4、寻找区分首token和其他token的神经元。
5、对于句子A，给定一固定前缀，然后结合一段重复n遍的token_1。对于句子B，由单独token_1组成。计算随着重复次数的增加，二者之间的L2范数差值。

### 研究点2
#### 研究目标
提出簇攻击，并给出相关的防御手段
#### 研究手段
根据前面的结论——第一层可以将token分为首token和非首token，利用这个机制寻找一组属于同一簇（属于首token子空间）的token。
防御方面：在预填充阶段，利用正常的激活值去覆盖后续token不正常的激活值。
## 实验效果
> 概括介绍实验结果，主要关注作者采取的实验指标、baseline和实际效果。
### 研究点1
这里的每一个点对应上面的研究手段
模型：LLaMA2-7B-HF
1、对于一个正常的句子（未给出），首token的attention score格外高（attention sink现象）。对于一个重复token（['the']）构成的句子，除了首token外，其他重复token的分数也很高。
2、随着['the'、'one'、'es']这三个token的重复次数的增加，token的L2范数越来越高，逐渐接近于BoS。
3、将部分神经元做零消融后，随着重复次数的增加，token的L2范数不再接近于BoS。输入：[’Another’, ’one’, ’bit’, ’es’, ’the’, ’dust’]分别重复1200次。
4、在llama2-7B-hf中，第一层MLP的gate neuron 912，可以将首token和非首token分到两个不同的线性子空间。
5、随着重复次数的增加，二者之间的L2范数差值越来越小，最后趋于零（token_1：the、dog、cat、bla）。
### 研究点2
成功找到了一组token。将这一组token构成句子，可以起到重复token攻击类似的效果，但又提高了隐蔽性。但是we did not quantify the success rate of the cluster attack（原文）。
作者提出的防御手段成功防御了簇攻击，但又不降低模型的表现（测试数据集：MMLU、HellaSwag、TruthfulQA、WinoGrande、AI2-ARC 测试模型：LLaMa-1-7B-HF、LLaMa-2-7B-HF、Mistral-7B-Instruct-v0.1）。


## 引用论文

> 关注论文的`Introduction`、`related work`和`experiment setup`部分，追踪查找作者列举的**关于资源消耗和输出长度异常**的相关研究。列出其基本信息

| 论文题目 | 发表刊物 | 时间 | 一句话概括研究领域和方向（例：llm 可解释性+防御） |
| -------- | -------- | ---- | ------------------------------------------------- |
|     无相关的，主题基本上都是数据窃取     |          |      |                                                   |
|          |          |      |                                                   |

## 被引论文

> 在 https://scholar.google.com/ 和 上进行当前论文搜索，以此查看所有引用本论文的研究，提取其中**关于资源消耗和输出长度异常**的相关研究。列出其基本信息

| 论文题目 | 发表刊物 | 时间 | 一句话概括研究领域和方向（例：llm 可解释性+防御） |
| -------- | -------- | ---- | ------------------------------------------------- |
|    无相关的，引用3，主题和数据窃取有关      |          |      |                                                   |
|          |          |      |                                                   |