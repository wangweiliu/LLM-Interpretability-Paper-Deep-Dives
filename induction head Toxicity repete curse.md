# 论文精读调研

> 调研论文，并撰写下列内容
> 调研人：王伟柳

## 论文名称

名称：Induction Head Toxicity Mechanistically Explains
Repetition Curse in Large Language Models

链接：https://arxiv.org/pdf/2505.13514v1
ps：最新版撤稿了
## 基础信息

| 工作类别（攻击/可解释性分析/防御）       | 具体信息 |
| ---------------------------------------- | -------- |
| 模态（llm/推理llm/多模态）               |llm          |
| 时间（年份，arxiv文章需要月份，如25.11） |2025          |
| 刊物                                     |arxiv         |
| 首作者                                   |Shuxun Wang|
| 首作单位                                 |Zhejiang University|
| 研究目标（输出增长/循环输出）                |循环输出          |
| 一句话方法概括                            |提出归纳头毒性理论来解释llm无限重复现象，并对归纳头进行对数缩放来缓解重复诅咒|

## 方法概述

> 1. 自己阅读文章，不要使用ai总结，有看不懂的地方可以问模型，但是不能依靠模型阅读。
> 2. 方法概述不需要过于详细的形式化公式，使用语言表述，以【研究目标-研究手段】为线路进行总结。其中研究手段可以稍作展开，分点按步骤进行撰写。
### 研究目标
从机理上解释llm存在重复诅咒现象，并给出一种无需重新训练的在推理阶段干预的方法，以缓解这一现象。

### 研究手段
1. 要求模型重复特定token一定次数，观察最后的重复次数与生成token之间的关系。

2. 使用activation patching技术识别出归纳头。作者后续通过公式推理证明了当归纳头占据主导地位时，会压制其他头的贡献，导致输出概率分布的熵值急剧下降。这种低熵状态会形成正反馈循环：模型越重复，归纳头接收到的信号越强，进而进一步强化重复模式，导致模型陷入死循环。

3. 在模型推理阶段，动态调整归纳头的输出，随着生成序列t的增加，对归纳头的输出除以$log(t+c)$。

## 实验效果
> 概括介绍实验结果，主要关注作者采取的实验指标、baseline和实际效果。
> 

模型：Qwen2.5-Instruct in 1.5B, 3B, and 7B parameter sizes、LLaMA3-Instruct with 8B parameters、Gemma2-it with 9B parameter.（All models mentioned above are instructed versions）

数据集：MMLU Pro、HumanEval+、ARC Challenge

1. 要求次数2～512，且为2的指数次。当重复次数要求大于8时，作者会预填充8个重复token。随着要求次数增多，模型的输出长度会突然暴增，模型超出要求重复次数的程度也越来越严重。此外，模型还表现出词汇敏感性，作者choose the last 1% token in Redpajama with n-gram counting，其现象不如上述明显。

2. 应用对数缩放后，模型能够精准地在达到目标次数（如512次）时停止。通用文本生成任务中，使用对数级去缩放的PPL与正常模型的非常接近，但线性缩放或常数缩放会严重损害模型性能。



## 引用论文

> 关注论文的`Introduction`、`related work`和`experiment setup`部分，追踪查找作者列举的**关于资源消耗和输出长度异常**的相关研究。列出其基本信息

| 论文题目 | 发表刊物 | 时间 | 一句话概括研究领域和方向（例：llm 可解释性+防御） |
| -------- | -------- | ---- | ------------------------------------------------- |



## 被引论文
被引 0 
> 在 https://scholar.google.com/ 和 上进行当前论文搜索，以此查看所有引用本论文的研究，提取其中**关于资源消耗和输出长度异常**的相关研究。列出其基本信息

| 论文题目 | 发表刊物 | 时间 | 一句话概括研究领域和方向（例：llm 可解释性+防御） |
| -------- | -------- | ---- | ------------------------------------------------- |
